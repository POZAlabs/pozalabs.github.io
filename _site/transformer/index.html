<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"> <meta name=viewport content="width=device-width, initial-scale=1"> <meta name=description content="Music & Deep Learning Startup"> <meta name=author content="포자랩스의 기술 블로그"> <!-- Begin Jekyll SEO tag v2.5.0 --> <title>Attention is all you need paper 뽀개기 | 포자랩스의 기술 블로그</title> <meta name="generator" content="Jekyll v3.7.4" /> <meta property="og:title" content="Attention is all you need paper 뽀개기" /> <meta name="author" content="hyemi" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Transformer paper review" /> <meta property="og:description" content="Transformer paper review" /> <link rel="canonical" href="http://localhost:4000/transformer/" /> <meta property="og:url" content="http://localhost:4000/transformer/" /> <meta property="og:site_name" content="포자랩스의 기술 블로그" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2018-09-15T00:00:00+09:00" /> <script type="application/ld+json"> {"url":"http://localhost:4000/transformer/","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/transformer/"},"headline":"Attention is all you need paper 뽀개기","dateModified":"2018-09-15T00:00:00+09:00","datePublished":"2018-09-15T00:00:00+09:00","author":{"@type":"Person","name":"hyemi"},"description":"Transformer paper review","@type":"BlogPosting","@context":"http://schema.org"}</script> <!-- End Jekyll SEO tag --> <link rel="apple-touch-icon-precomposed" sizes="57x57" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-57x57.png" /> <link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-114x114.png" /> <link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-72x72.png" /> <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-144x144.png" /> <link rel="apple-touch-icon-precomposed" sizes="60x60" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-60x60.png" /> <link rel="apple-touch-icon-precomposed" sizes="120x120" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-120x120.png" /> <link rel="apple-touch-icon-precomposed" sizes="76x76" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-76x76.png" /> <link rel="apple-touch-icon-precomposed" sizes="152x152" href="http://localhost:4000/assets/images/favicon/apple-touch-icon-152x152.png" /> <link rel="icon" type="image/png" href="http://localhost:4000/assets/images/favicon/favicon-196x196.png" sizes="196x196" /> <link rel="icon" type="image/png" href="http://localhost:4000/assets/images/favicon/favicon-96x96.png" sizes="96x96" /> <link rel="icon" type="image/png" href="http://localhost:4000/assets/images/favicon/favicon-32x32.png" sizes="32x32" /> <link rel="icon" type="image/png" href="http://localhost:4000/assets/images/favicon/favicon-16x16.png" sizes="16x16" /> <link rel="icon" type="image/png" href="http://localhost:4000/assets/images/favicon/favicon-128.png" sizes="128x128" /> <meta name="application-name" content="&nbsp;"/> <meta name="msapplication-TileColor" content="#FFFFFF" /> <meta name="msapplication-TileImage" content="mstile-144x144.png" /> <meta name="msapplication-square70x70logo" content="mstile-70x70.png" /> <meta name="msapplication-square150x150logo" content="mstile-150x150.png" /> <meta name="msapplication-wide310x150logo" content="mstile-310x150.png" /> <meta name="msapplication-square310x310logo" content="mstile-310x310.png" /> <link rel="canonical" href="http://localhost:4000/transformer/"> <link rel="alternate" type="application/rss+xml" title="" href="http://localhost:4000/feed.xml" /> <style> @charset "UTF-8"; /*! normalize.css v4.1.1 | MIT License | github.com/necolas/normalize.css */ /** 1. Change the default font family in all browsers (opinionated). 2. Prevent adjustments of font size after orientation changes in IE and iOS. */ html { font-family: sans-serif; /* 1 */ -ms-text-size-adjust: 100%; /* 2 */ -webkit-text-size-adjust: 100%; /* 2 */ } /** Remove the margin: in all browsers (opinionated). */ body { margin: 0; } /* HTML5 display: definitions ========================================================================== */ /** Add the correct display: in IE 9-. 1. Add the correct display: in Edge, IE, and Firefox. 2. Add the correct display: in IE. */ article, aside, details, figcaption, figure, footer, header, main, menu, nav, section, summary { /* 1 */ display: block; } /** Add the correct display: in IE 9-. */ audio, canvas, progress, video { display: inline-block; } /** Add the correct display: in iOS 4-7. */ audio:not([controls]) { display: none; height: 0; } /** Add the correct vertical alignment in Chrome, Firefox, and Opera. */ progress { vertical-align: baseline; } /** Add the correct display: in IE 10-. 1. Add the correct display: in IE. */ template, [hidden] { display: none; } /* Links ========================================================================== */ /** 1. Remove the gray background: on active links in IE 10. 2. Remove gaps in links underline in iOS 8+ and Safari 8+. */ a { background-color: transparent; /* 1 */ -webkit-text-decoration-skip: objects; /* 2 */ } /** Remove the outline on focused links when they are also active or hovered in all browsers (opinionated). */ a:active, a:hover { outline-width: 0; } /* Text-level semantics ========================================================================== */ /** 1. Remove the bottom border: in Firefox 39-. 2. Add the correct text decoration in Chrome, Edge, IE, Opera, and Safari. */ abbr[title] { border-bottom: none; /* 1 */ text-decoration: underline; /* 2 */ text-decoration: underline dotted; /* 2 */ } /** Prevent the duplicate application of `bolder` by the next rule in Safari 6. */ b, strong { font-weight: inherit; } /** Add the correct font weight in Chrome, Edge, and Safari. */ b, strong { font-weight: bolder; } /** Add the correct font style in Android 4.3-. */ dfn { font-style: italic; } /** Correct the font size and margin: on `h1` elements within `section` and `article` contexts in Chrome, Firefox, and Safari. */ h1 { font-size: 2em; margin: 0.67em 0; } /** Add the correct background: and color: in IE 9-. */ mark { background-color: #ff0; color: #000; } /** Add the correct font size in all browsers. */ small { font-size: 80%; } /** Prevent `sub` and `sup` elements from affecting the line height: in all browsers. */ sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; } sub { bottom: -0.25em; } sup { top: -0.5em; } /* Embedded content ========================================================================== */ /** Remove the border: on images inside links in IE 10-. */ img { border-style: none; } /** Hide the overflow in IE. */ svg:not(:root) { overflow: hidden; } /* Grouping content ========================================================================== */ /** 1. Correct the inheritance and scaling of font size in all browsers. 2. Correct the odd `em` font sizing in all browsers. */ code, kbd, pre, samp { font-family: monospace, monospace; /* 1 */ font-size: 1em; /* 2 */ } /** Add the correct margin: in IE 8. */ figure { margin: 1em 40px; } /** 1. Add the correct box sizing in Firefox. 2. Show the overflow in Edge and IE. */ hr { box-sizing: content-box; /* 1 */ height: 0; /* 1 */ overflow: visible; /* 2 */ } /* Forms ========================================================================== */ /** 1. Change font properties to `inherit` in all browsers (opinionated). 2. Remove the margin: in Firefox and Safari. */ button, input, select, textarea { font: inherit; /* 1 */ margin: 0; /* 2 */ } /** Restore the font weight unset by the previous rule. */ optgroup { font-weight: bold; } /** Show the overflow in IE. 1. Show the overflow in Edge. */ button, input { /* 1 */ overflow: visible; } /** Remove the inheritance of text transform in Edge, Firefox, and IE. 1. Remove the inheritance of text transform in Firefox. */ button, select { /* 1 */ text-transform: none; } /** 1. Prevent a WebKit bug where (2) destroys native `audio` and `video` controls in Android 4. 2. Correct the inability to style clickable types in iOS and Safari. */ button, html [type="button"], [type="reset"], [type="submit"] { -webkit-appearance: button; /* 2 */ } /** Remove the inner border: and padding: in Firefox. */ button::-moz-focus-inner, [type="button"]::-moz-focus-inner, [type="reset"]::-moz-focus-inner, [type="submit"]::-moz-focus-inner { border-style: none; padding: 0; } /** Restore the focus styles unset by the previous rule. */ button:-moz-focusring, [type="button"]:-moz-focusring, [type="reset"]:-moz-focusring, [type="submit"]:-moz-focusring { outline: 1px dotted ButtonText; } /** Change the border, margin, and padding: in all browsers (opinionated). */ fieldset { border: 1px solid #c0c0c0; margin: 0 2px; padding: 0.35em 0.625em 0.75em; } /** 1. Correct the text wrapping in Edge and IE. 2. Correct the color: inheritance from `fieldset` elements in IE. 3. Remove the padding: so developers are not caught out when they zero out `fieldset` elements in all browsers. */ legend { box-sizing: border-box; /* 1 */ color: inherit; /* 2 */ display: table; /* 1 */ max-width: 100%; /* 1 */ padding: 0; /* 3 */ white-space: normal; /* 1 */ } /** Remove the default vertical scrollbar in IE. */ textarea { overflow: auto; } /** 1. Add the correct box sizing in IE 10-. 2. Remove the padding: in IE 10-. */ [type="checkbox"], [type="radio"] { box-sizing: border-box; /* 1 */ padding: 0; /* 2 */ } /** Correct the cursor style of increment and decrement buttons in Chrome. */ [type="number"]::-webkit-inner-spin-button, [type="number"]::-webkit-outer-spin-button { height: auto; } /** 1. Correct the odd appearance in Chrome and Safari. 2. Correct the outline style in Safari. */ [type="search"] { -webkit-appearance: textfield; /* 1 */ outline-offset: -2px; /* 2 */ } /** Remove the inner padding: and cancel buttons in Chrome and Safari on OS X. */ [type="search"]::-webkit-search-cancel-button, [type="search"]::-webkit-search-decoration { -webkit-appearance: none; } /** Correct the text style of placeholders in Chrome, Edge, and Safari. */ ::-webkit-input-placeholder { color: inherit; opacity: 0.54; } /** 1. Correct the inability to style clickable types in iOS and Safari. 2. Change font properties to `inherit` in Safari. */ ::-webkit-file-upload-button { -webkit-appearance: button; /* 1 */ font: inherit; /* 2 */ } /* GitHub style for Pygments syntax highlighter, for use with Jekyll Courtesy of GitHub.com */ .highlight .c { color: #999988; font-style: italic; } .highlight .err { color: #a61717; background-color: #e3d2d2; } .highlight .k { font-weight: bold; } .highlight .o { font-weight: bold; } .highlight .cm { color: #999988; font-style: italic; } .highlight .cp { color: #999999; font-weight: bold; } .highlight .c1 { color: #999988; font-style: italic; } .highlight .cs { color: #999999; font-weight: bold; font-style: italic; } .highlight .gd { color: #000000; background-color: #ffdddd; } .highlight .gd .x { color: #000000; background-color: #ffaaaa; } .highlight .ge { font-style: italic; } .highlight .gr { color: #aa0000; } .highlight .gh { color: #999999; } .highlight .gi { color: #000000; background-color: #ddffdd; } .highlight .gi .x { color: #000000; background-color: #aaffaa; } .highlight .go { color: #888888; } .highlight .gp { color: #555555; } .highlight .gs { font-weight: bold; } .highlight .gu { color: #800080; font-weight: bold; } .highlight .gt { color: #aa0000; } .highlight .kc { font-weight: bold; } .highlight .kd { font-weight: bold; } .highlight .kn { font-weight: bold; } .highlight .kp { font-weight: bold; } .highlight .kr { font-weight: bold; } .highlight .kt { color: #445588; font-weight: bold; } .highlight .m { color: #009999; } .highlight .s { color: #dd1144; } .highlight .n { color: #333333; } .highlight .na { color: teal; } .highlight .nb { color: #0086b3; } .highlight .nc { color: #445588; font-weight: bold; } .highlight .no { color: teal; } .highlight .ni { color: purple; } .highlight .ne { color: #990000; font-weight: bold; } .highlight .nf { color: #990000; font-weight: bold; } .highlight .nn { color: #555555; } .highlight .nt { color: navy; } .highlight .nv { color: teal; } .highlight .ow { font-weight: bold; } .highlight .w { color: #bbbbbb; } .highlight .mf { color: #009999; } .highlight .mh { color: #009999; } .highlight .mi { color: #009999; } .highlight .mo { color: #009999; } .highlight .sb { color: #dd1144; } .highlight .sc { color: #dd1144; } .highlight .sd { color: #dd1144; } .highlight .s2 { color: #dd1144; } .highlight .se { color: #dd1144; } .highlight .sh { color: #dd1144; } .highlight .si { color: #dd1144; } .highlight .sx { color: #dd1144; } .highlight .sr { color: #009926; } .highlight .s1 { color: #dd1144; } .highlight .ss { color: #990073; } .highlight .bp { color: #999999; } .highlight .vc { color: teal; } .highlight .vg { color: teal; } .highlight .vi { color: teal; } .highlight .il { color: #009999; } .highlight .gc { color: #999; background-color: #EAF2F5; } body, html { font-size: 62.5%; } body { line-height: 1; font: 16px "Helvetica Neue", Helvetica, Arial, sans-serif; color: #666; } h1, h2, h3, h4 { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; color: #222; -webkit-font-smoothing: antialiased; text-rendering: optimizeLegibility; } h1 { font-size: 3rem; letter-spacing: -1px; color: #222; font-weight: 700; } h2 { font-size: 2.2rem; } h3 { font-size: 2rem; } h4 { font-size: 1.6rem; } a { color: #4b0082; text-decoration: underline; } p { line-height: 1.7; color: #666; font-weight: 300; margin-bottom: 20px; letter-spacing: 0.4px; } @media only screen and (max-width: 400px) { p { letter-spacing: 0.2px; } } strong { font-weight: 400; color: #000; } ul li, ol li { line-height: 2.4rem; font-weight: 300; color: #666; } img, pre, iframe { max-width: 100%; } img, pre { border-radius: 4px; } figcaption { position: relative; top: -20px; left: 0; right: 0; margin: 0 auto; width: 100%; text-align: center; font-size: 1.3rem; color: #aaa; font-weight: 300; } @media only screen and (max-width: 400px) { figcaption { font-size: 1.2rem; } } blockquote { padding-left: 15px; border-left: 3px solid #eee; } hr { border: none; height: 1px; margin: 40px auto; background: #eee; width: 100%; } figure.highlight { width: 100%; margin: 0; } code, tt { padding: 1px 0; font-family: "Consolas", Liberation Mono, Menlo, Courier, monospace; font-size: 12px; line-height: 20px; background: #fff; border-radius: 2px; border-radius: 2px; } pre { box-sizing: border-box; margin: 0 0 1.75em 0; width: 100%; padding: 5px 10px; font-family: "Consolas", Liberation Mono, Menlo, Courier, monospace; font-size: 1.2rem; line-height: 2rem; overflow: auto; background: #fff; border: 1px solid #ededed; border-radius: 2px; } .wrapper-normal, .wrapper-large { height: 100%; width: 96%; margin: 0 auto; } @media only screen and (max-width: 400px) { .wrapper-normal, .wrapper-large { width: 88%; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .wrapper-normal, .wrapper-large { width: 88%; } } .wrapper-normal { max-width: 560px; } .wrapper-large { max-width: 810px; } /* general helpers */ .text-center { text-align: center; } .clearfix:before, .clearfix:after { content: ""; display: table; } .clearfix:after { clear: both; } /* animations */ .animated { animation: fade-in-down 0.6s; animation-delay: 0.3s; animation-fill-mode: both; } @keyframes fade-in-down { 0% { opacity: 0; transform: translateY(-10px); } 100% { opacity: 1; transform: translateY(0); } } .home, .blog, .projects { margin-top: 125px; } .home > .list, .blog > .list, .projects > .list { border-top: 1px solid #ededed; margin-top: 30px; padding-top: 40px; position: relative; } .home > .list:before, .blog > .list:before, .projects > .list:before { display: block; content: " "; width: 7px; height: 7px; border: #ededed 1px solid; position: absolute; top: -5px; left: 50%; margin-left: -5px; background: #FFF; box-shadow: #FFF 0 0 0 5px; border-radius: 3px; } .home > .list > .item, .blog > .list > .item, .projects > .list > .item { display: block; width: 95%; margin: 0 auto; } .home > .list > .item > .url, .blog > .list > .item > .url, .projects > .list > .item > .url { width: 100%; display: block; padding: 20px 0; text-decoration: none; } .home > .list > .item > .url > .title, .blog > .list > .item > .url > .title, .projects > .list > .item > .url > .title { margin: 0; width: 75%; font-weight: 500; transition: all ease-in-out 0.2s; } .home > .list > .item:hover > .url > .title, .blog > .list > .item:hover > .url > .title, .projects > .list > .item:hover > .url > .title { color: #4b0082; } .home > .list aside, .blog > .list aside, .projects > .list aside { position: relative; top: 2px; margin: 0; width: 25%; float: right; font-weight: 300; color: #aaa; text-align: right; transition: all ease-in-out 0.2s; } .home > .list .item:hover .url aside, .blog > .list .item:hover .url aside, .projects > .list .item:hover .url aside { color: #666; } .blog > .list > .item > .url > .title, .projects > .list > .item > .url > .title { display: inline; } .blog > .list > .item > .url > .emoji, .projects > .list > .item > .url > .emoji { display: inline; position: relative; top: -4px; margin-right: 10px; } .page { margin-top: 125px; } .page > h1 { text-align: center; margin-bottom: 6rem; } .about img { width: 50%; margin: 0 auto; display: block; } .post { margin-top: 125px; } .post > .title { text-align: center; margin-bottom: 3rem; } .post > .date, .post > .post-tags { color: #aaa; font-weight: 300; font-size: 1.4rem; text-transform: uppercase; text-align: center; display: block; margin-bottom: 6rem; letter-spacing: 1px; -webkit-font-smoothing: antialiased; text-rendering: optimizeLegibility; } .post > .date { margin-bottom: 2rem; } .post > .post-tags > .item { padding: 2px 8px; border-radius: 3px; font-size: 1.1rem; background: #ededed; color: #666; letter-spacing: 1px; margin: 3px 1px; text-decoration: none; display: inline-block; } .post > h2, .post > h3, .post > h4 { margin-top: 40px; } .post > h2 a, .post > h3 a, .post > h4 a { text-decoration: none; } .post > .title-image { max-height: 120px; display: block; margin: 0 auto; } .post > .blog-navigation { font-size: 1.4rem; display: block; width: auto; overflow: hidden; } .post > .blog-navigation a { display: block; width: 50%; float: left; margin: 1em 0; } .post > .blog-navigation .next { text-align: right; } .tags { margin-top: 125px; } .tags > .list { border-top: 1px solid #ededed; margin-top: 30px; padding-top: 40px; position: relative; } .tags > .list:before { display: block; content: " "; width: 7px; height: 7px; border: #ededed 1px solid; position: absolute; top: -5px; left: 50%; margin-left: -5px; background: #FFF; box-shadow: #FFF 0 0 0 5px; border-radius: 3px; } .tags > .list > .item { font-weight: 300; text-transform: uppercase; text-align: center; margin-bottom: 6rem; -webkit-font-smoothing: antialiased; text-rendering: optimizeLegibility; padding: 3px 9px; border-radius: 3px; font-size: 1.3rem; background: #ededed; color: #666; letter-spacing: 1px; margin: 0 0.5rem 1rem; text-decoration: none; display: inline-block; } .tag-list > .list { padding: 0; } .tag-list > .list > .item { display: block; width: 80%; margin: 0 10%; } .tag-list > .list > .item > .url { width: 100%; height: 100%; display: block; padding: 20px 0; text-decoration: none; } .tag-list > .list > .item > .url > .title { margin: 0; width: 75%; font-weight: 400; transition: all ease-in-out 0.2s; font-size: 1.6rem; } .tag-list > .list > .item:hover > .url > .title { color: #4b0082; } .tag-list > .list aside { position: relative; top: 2px; margin: 0; width: 25%; float: right; font-weight: 300; color: #aaa; text-align: right; transition: all ease-in-out 0.2s; font-size: 1.6rem; } .tag-list > .list .item:hover .url aside { color: #666; } .author { padding: 3rem 0; border-bottom: 1px solid #ededed; border-top: 1px solid #ededed; max-width: 100%; margin: 4rem auto 0; } .author > .toleft > .selfie { width: 90%; border-radius: 100%; } .author > .toright > .name, .author > .toright > .bio { width: 60%; display: inline-block; } .author > .toright > .name { font-size: 1.5rem; font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: 500; margin: 6px 0 0; } @media only screen and (max-width: 400px) { .author > .toright > .name { width: 100%; display: block; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .author > .toright > .name { width: 100%; display: block; } } .author > .toright > .bio { font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; font-weight: 300; color: #aaa; font-size: 1.3rem; text-align: justify; line-height: 1.5; margin: 0; } @media only screen and (max-width: 400px) { .author > .toright > .bio { width: 100%; display: block; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .author > .toright > .bio { width: 100%; display: block; } } .author > .toleft { width: 10%; display: inline-block; } @media only screen and (max-width: 400px) { .author > .toleft { width: 20%; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .author > .toleft { width: 20%; } } .author > .toright { width: 89%; display: inline-block; vertical-align: top; } @media only screen and (max-width: 400px) { .author > .toright { width: 78%; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .author > .toright { width: 78%; } } .no-disqus { border-bottom: none; padding-bottom: 0; } .disqus { margin: 0 auto; max-width: 100%; padding: 40px 0; } .footer-main { border-top: 1px solid #ededed; padding: 40px 0; margin: 40px 0 0; font-size: 1.3rem; color: #aaa; font-weight: 300; text-align: center; position: relative; } .footer-main:before { display: block; content: " "; width: 7px; height: 7px; border: #ededed 1px solid; position: absolute; top: -5px; left: 50%; margin-left: -5px; background: #FFF; box-shadow: #FFF 0 0 0 5px; border-radius: 3px; } .footer-main > .link { display: inline; } .footer-main > .link > .icon { width: 15px !important; fill: #aaa !important; transition: ease-in-out all 0.3s; position: relative; top: 3px; display: inherit; margin: 0; } .footer-main > .link > .icon:hover { fill: #4b0082 !important; } .footer-main > .extra { color: #aaa; margin-top: 0; } .footer-main > .extra > .link { color: #222; text-decoration: none; border-bottom: 1px solid transparent; transition: ease-in-out all 0.3s; padding-bottom: 1px; } .footer-main > .extra > .link:hover { border-color: #aaa; } .header-home { display: block; margin: 0 auto; text-align: center; position: relative; z-index: 99; } .header-home > .link > .selfie { width: 125px; margin-bottom: 25px; border-radius: 100%; transition: all 0.2s; box-shadow: 0; opacity: 1; } .header-home > .link > .selfie:hover { box-shadow: 0 0px 4px 0 rgba(0, 0, 0, 0.18), 0 0px 12px 0 rgba(0, 0, 0, 0.15); opacity: 0.8; } .header-home > .title { font-size: 4rem; margin: 0 0 13px; } .header-home > .description { font-size: 1.85rem; font-weight: 300; font-style: normal; color: #aaa; width: 70%; margin: 0 auto 30px; } .header-home > .description a { font-weight: 200; } .nav > .list, .nav-home > .list { list-style: none; margin: 0; padding: 0 13px 0; } .nav > .list > .item, .nav-home > .list > .item { display: inline-block; } .nav > .list > .item > .link, .nav-home > .list > .item > .link { display: inline-block; font-weight: 300; font-size: 1.4rem; padding: 20px 10px; text-decoration: none; } .nav { position: absolute; right: 0; top: 0; } .nav > .list { padding: 0 13px 0; } .nav > .list > .item > .link { font-size: 1.4rem; padding: 20px 10px; } .nav-home { margin-top: 40px; text-align: center; } .nav-home > .list { padding: 0; } .nav-home > .list > .item > .link { font-size: 2rem; padding: 7px 15px; margin: 0; border-radius: 4%; transition: all 0.4s ease-in-out; width: 70px; } .nav-home > .list > .item > .link:hover { color: #666; } .evidence { background-image: linear-gradient(to bottom, rgba(39, 243, 106, 0.15), rgba(39, 243, 106, 0.15)); color: beta; } .star > .url > .title { width: auto !important; display: inline; background-image: linear-gradient(rgba(39, 243, 106, 0.15), rgba(39, 243, 106, 0.15)); } .twitter-tweet { margin: 10px auto; } .icon { display: inline-block; width: 17px; height: 17px; fill: #000; text-align: center; color: #000; margin: 7px auto; } .caption { position: relative; top: 1rem; left: 0; right: 0; margin: 0 auto; width: 100%; text-align: center; font-size: 1.3rem; } .bigger-image { min-width: 130%; margin: 5rem 0 5rem -15%; } @media only screen and (max-width: 400px) { .bigger-image { min-width: 114%; margin: 2rem 0 2rem -7%; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .bigger-image { min-width: 114%; margin: 2rem 0 2rem -7%; } } .breaker { height: 1px; margin: 6rem auto; width: 100%; } .breaker:before { content: "• • •"; width: 100%; text-align: center; display: block; color: #aaa; letter-spacing: 4px; position: relative; top: -8px; } .pagination { width: 95%; margin: 3rem auto 0; text-align: center; } .pagination > .page_number { display: inline-block; font-size: 1.3rem; } .pagination > .previous, .pagination > .next { display: inline-block; font-size: 1.8rem; position: relative; top: 1px; padding: 1px 9px; } .pagination > .hidden { visibility: hidden; } .related { margin: 10rem 0 0rem; } .share { float: right; width: 40%; display: inline; text-align: right; position: relative; top: -10px; } @media only screen and (max-width: 400px) { .share { width: 100%; display: block; top: 0; text-align: left; float: none; margin-top: 5px; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .share { width: 100%; display: block; top: 0; text-align: left; float: none; margin-top: 5px; } } .share > .twitter, .share > .facebook, .share > .google-plus, .share > .linkedin, .share > .reddit { display: inline; vertical-align: middle; font-size: 13px; font-weight: 700; color: #fff; padding: 6px 10px; border-radius: 3px; margin-left: 5px; text-decoration: none; } @media only screen and (max-width: 400px) { .share > .twitter, .share > .facebook, .share > .google-plus, .share > .linkedin, .share > .reddit { margin: 0 5px 10px 0; } } @media only screen and (min-width: 400px) and (max-width: 1050px) { .share > .twitter, .share > .facebook, .share > .google-plus, .share > .linkedin, .share > .reddit { margin: 0 5px 10px 0; } } .share > .twitter { background: #4fafed; } .share > .facebook { background: #4361b3; } .share > .google-plus { background: #dd4b39; } .share > .linkedin { background: #0077b5; } .share > .reddit { background: #ff4500; } .share svg { fill: #fff; position: relative; top: 3px; margin: 0; margin-right: 4px; display: inherit; } @media only screen and (min-width: 780px) { .side-by-side { width: 130%; margin: 6rem 0 6rem -15%; } } @media only screen and (max-width: 780px) { .side-by-side { width: 100%; margin: 4rem 0; } } .side-by-side > .toleft, .side-by-side > .toright { display: inline-block; width: 47.5%; } @media only screen and (max-width: 780px) { .side-by-side > .toleft img, .side-by-side > .toright img { text-align: center; display: block; margin: 0 auto; } } @media only screen and (min-width: 780px) { .side-by-side > .toleft { margin-right: 2%; } } @media only screen and (max-width: 780px) { .side-by-side > .toleft { width: 100%; margin: 0 0 4rem 0; } } @media only screen and (min-width: 780px) { .side-by-side > .toright { margin-left: 2%; vertical-align: top; } } @media only screen and (max-width: 780px) { .side-by-side > .toright { width: 100%; margin: 0 0 4rem 0; } } .side-by-side > .toleft > p, .side-by-side > .toright > p { margin: 0 0 4rem 0; } @media only screen and (max-width: 780px) { .side-by-side > .toleft > p, .side-by-side > .toright > p { margin: 0; } } .social-links { margin-top: 20px; } .social-links > .link { margin: 0; text-decoration: none; position: relative; display: inline-block; height: 35px; width: 35px; } .social-links > .link:hover > .icon { fill: #4b0082; } .social-links > .link:hover:before { opacity: 1; display: inline-block; transform: translate3d(0, 0, 0); white-space: nowrap; } .social-links > .link:before { content: attr(data-title); display: none; position: absolute; bottom: -34px; left: -6px; margin: 0 auto; font-size: 13px; padding: 3px 10px; background: #222; color: #fff; border-radius: 2px; height: 22px; line-height: 22px; opacity: 0; transition: opacity 150ms linear, transform 150ms linear, -webkit-transform 150ms linear; transform: translate3d(0, -8px, 0); z-index: 99; } .social-links > .link:after { content: ""; position: absolute; top: 35px; left: 13px; width: 0; height: 0; border-left: 5px solid transparent; border-right: 5px solid transparent; border-bottom: 6px solid #222; opacity: 0; transition: opacity 150ms linear, transform 150ms linear, -webkit-transform 150ms linear; transform: translate3d(0, -8px, 0); z-index: 100; } .social-links > .link:hover:after { opacity: 1; transform: translate3d(0, 0, 0); } .social-links > .icon { transition: all ease-in-out 0.2s; } .spoiler { position: relative; } .spoiler:before { content: ""; background-color: #fafae0; position: absolute; top: 0; bottom: 0; left: 0; right: 0; z-index: 50; } .spoiler:hover:before { display: none; } </style> </head> <body> <div class="wrapper-normal"> <div class="post"> <nav class="nav"> <ul class="list"> <li class="item"> <a class="link" href="http://localhost:4000/">Home</a> </li> <li class="item"> <a class="link" href="http://localhost:4000/blog">Blog</a> </li> <li class="item"> <a class="link" href="http://localhost:4000/about">About</a> </li> </ul> </nav> <h1 class="title">Attention is all you need paper 뽀개기</h1> <span class="date"> <time datetime="15-09-2018">Saturday. September 15, 2018</time> - <span class="reading-time" title="Estimated read time"> 11 mins </span> </span> <div class="post-tags"> <a class="item" href="http://localhost:4000/tags/#transformer">transformer</a> <a class="item" href="http://localhost:4000/tags/#attention">attention</a> <a class="item" href="http://localhost:4000/tags/#attention-is-all-you-need">attention is all you need</a> <a class="item" href="http://localhost:4000/tags/#paper">paper</a> <a class="item" href="http://localhost:4000/tags/#review">review</a> </div> <p>이번 포스팅에서는 포자랩스에서 핵심적으로 쓰고 있는 모델인 <strong>transformer</strong>의 논문을 요약하면서 추가적인 기법들도 설명드리겠습니다.</p> <p>​</p> <h1 id="why">Why?</h1> <h2 id="long-term-dependency-problem">Long-term dependency problem</h2> <ul> <li>sequence data를 처리하기 위해 이전까지 많이 쓰이던 model은 recurrent model이었습니다. recurrent model은 t번째에 대한 output을 만들기 위해, t번째 input과 t-1번째 hidden state를 이용했습니다. 이렇게 한다면 자연스럽게 문장의 순차적인 특성이 유지됩니다. 문장을 쓸 때 뒤의 단어부터 쓰지 않고 처음부터 차례차례 쓰는 것과 마찬가지인것입니다.</li> <li>하지만 recurrent model의 경우 많은 개선점이 있었음에도 long-term dependency에 취약하다는 단점이 있었습니다. 예를 들어, “저는 언어학을 좋아하고, 인공지능중에서도 딥러닝을 배우고 있고 자연어 처리에 관심이 많습니다.”라는 문장을 만드는 게 model의 task라고 해봅시다. 이때 ‘자연어’라는 단어를 만드는데 ‘언어학’이라는 단어는 중요한 단서입니다.</li> <li>그러나, 두 단어 사이의 거리가 가깝지 않으므로 model은 앞의 ‘언어학’이라는 단어를 이용해 자연어’라는 단어를 만들지 못하고, 언어학 보다 가까운 단어인 ‘딥러닝’을 보고 ‘이미지’를 만들 수도 있는 거죠. 이처럼, 어떤 정보와 다른 정보 사이의 거리가 멀 때 해당 정보를 이용하지 못하는 것이 <strong>long-term dependency problem</strong>입니다.</li> <li>recurrent model은 순차적인 특성이 유지되는 뛰어난 장점이 있었음에도, long-term dependency problem이라는 단점을 가지고 있었습니다.</li> <li>이와 달리 transformer는 recurrence를 사용하지 않고 대신 <strong>attention mechanism</strong>만을 사용해 input과 output의 dependency를 포착해냈습니다.</li> </ul> <p>​</p> <h2 id="parallelization">Parallelization</h2> <ul> <li>recurrent model은 학습 시, t번째 hidden state를 얻기 위해서 t-1번째 hidden state가 필요했습니다. 즉, 순서대로 계산될 필요가 있었습니다. 그래서 병렬 처리를 할 수 없었고 계산 속도가 느렸습니다.</li> <li>하지만 transformer에서는 학습 시 encoder에서는 각각의 position에 대해, 즉 각각의 단어에 대해 attention을 해주기만 하고, decoder에서는 masking 기법을 이용해 병렬 처리가 가능하게 됩니다. (masking이 어떤 것인지는 이후에 설명해 드리겠습니다)</li> </ul> <p>​</p> <h1 id="model-architecture">Model Architecture</h1> <h2 id="encoder-and-decoder-structure">Encoder and Decoder structure</h2> <p><img src="/assets/images/encoder-decoder.png" alt="encoder-decoder" /></p> <ul> <li>encoder는 input sequence <script type="math/tex">(x_1, ..., x_n)</script>에 대해 다른 representation인 <script type="math/tex">z = (z_1, ..., z_n)</script>으로 바꿔줍니다.</li> <li>decoder는 <strong>z</strong>를 받아, output sequence <script type="math/tex">(y_1, ... , y_n)</script>를 하나씩 만들어냅니다.</li> <li>각각의 step에서 다음 symbol을 만들 때 이전에 만들어진 output(symbol)을 이용합니다. 예를 들어, “저는 사람입니다.”라는 문장에서 ‘사람입니다’를 만들 때, ‘저는’이라는 symbol을 이용하는 거죠. 이런 특성을 <em>auto-regressive</em> 하다고 합니다.</li> </ul> <p>​</p> <h2 id="encoder-and-decoder-stacks">Encoder and Decoder stacks</h2> <p><img src="/assets/images/archi2.png" alt="architecture" /></p> <h3 id="encoder">Encoder</h3> <ul> <li>N개의 동일한 layer로 구성돼 있습니다. input $x$가 첫 번째 layer에 들어가게 되고, <script type="math/tex">layer(x)</script>가 다시 layer에 들어가는 식입니다.</li> <li>그리고 각각의 layer는 두 개의 sub-layer, <strong>multi-head self-attention mechanism</strong>과 <strong>position-wise fully connected feed-forward network</strong>를 가지고 있습니다.</li> <li>이때 두 개의 sub-layer에 <strong>residual connection</strong>을 이용합니다. residual connection은 input을 output으로 그대로 전달하는 것을 말합니다. 이때 sub-layer의 output dimension을 embedding dimension과 맞춰줍니다. <script type="math/tex">x+Sublayer(x)</script>를 하기 위해서, 즉 residual connection을 하기 위해서는 두 값의 차원을 맞춰줄 필요가 있습니다. 그 후에 <strong>layer normalization</strong>을 적용합니다.</li> </ul> <h3 id="decoder">Decoder</h3> <ul> <li>역시 N개의 동일한 layer로 이루어져 있습니다.</li> <li>encoder와 달리 encoder의 결과에 multi-head attention을 수행할 sub-layer를 추가합니다.</li> <li>마찬가지로 sub-layer에 <strong>residual connection</strong>을 사용한 뒤, <strong>layer normalization</strong>을 해줍니다.</li> <li> <p>decoder에서는 encoder와 달리 <em>순차적으로</em> 결과를 만들어내야 하기 때문에, self-attention을 변형합니다. 바로 <strong>masking</strong>을 해주는 것이죠. masking을 통해, position <script type="math/tex">i</script> 보다 이후에 있는 position에 attention을 주지 못하게 합니다. 즉, position <script type="math/tex">i</script>에 대한 예측은 미리 알고 있는 output들에만 의존을 하는 것입니다.</p> <p><img src="/assets/images/masking.png" alt="masking" /></p> </li> <li>위의 예시를 보면, <strong>a</strong>를 예측할 때는 <strong>a</strong>이후에 있는 <strong>b,c</strong>에는 attention이 주어지지 않는 것입니다. 그리고 <strong>b</strong>를 예측할 때는 <strong>b</strong>이전에 있는 <strong>a</strong>만 attention이 주어질 수 있고 이후에 있는 <strong>c</strong>는 attention이 주어지지 않는 것이죠.</li> </ul> <p>​</p> <h2 id="embeddings-and-softmax">Embeddings and Softmax</h2> <ul> <li>embedding 값을 고정시키지 않고, 학습을 하면서 embedding값이 변경되는 learned embedding을 사용했습니다. 이때 input과 output은 같은 embedding layer를 사용합니다.</li> <li>또한 decoder output을 다음 token의 확률로 바꾸기 위해 learned linear transformation과 softmax function을 사용했습니다. learned linear transformation을 사용했다는 것은 decoder output에 weight matrix <script type="math/tex">W</script>를 곱해주는데, 이때 <script type="math/tex">W</script>가 학습된다는 것입니다.</li> </ul> <p>​</p> <h2 id="attention">Attention</h2> <ul> <li><em>attention</em>은 단어의 의미처럼 특정 정보에 좀 더 주의를 기울이는 것입니다.</li> <li>예를 들어 model이 수행해야 하는 task가 번역이라고 해봅시다. source는 영어이고 target은 한국어입니다. “Hi, my name is poza.”라는 문장과 대응되는 “안녕, 내 이름은 포자야.”라는 문장이 있습니다. model이 <em>이름은</em>이라는 token을 decode할 때, source에서 가장 중요한 것은 <em>name</em>입니다.</li> <li>그렇다면, source의 모든 token이 비슷한 중요도를 갖기 보다는 <em>name</em>이 더 큰 중요도를 가지면 되겠죠. 이때, 더 큰 중요도를 갖게 만드는 방법이 바로 <strong>attention</strong>입니다.</li> </ul> <h3 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h3> <p><img src="/assets/images/sdpa.PNG" alt="scaled-dot-product-attention" /></p> <ul> <li>해당 논문의 attention을 <strong>Scaled Dot-Product Attention</strong>이라고 부릅니다. 수식을 살펴보면 이렇게 부르는 이유를 알 수 있습니다.</li> </ul> <script type="math/tex; mode=display">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script> <ul> <li>먼저 input은 <script type="math/tex">d_k</script> dimension의 query와 key들, <script type="math/tex">d_v</script> dimension의 value들로 이루어져 있습니다.</li> <li>이때 모든 query와 key에 대한 dot-product를 계산하고 각각을 <script type="math/tex">\sqrt{d_k}</script>로 나누어줍니다. <em>dot-product</em>를 하고 <script type="math/tex">\sqrt{d_k}</script>로 scaling을 해주기 때문에 <strong>Scaled Dot-Product Attention</strong>인 것입니다. 그리고 여기에 softmax를 적용해 value들에 대한 weights를 얻어냅니다.</li> <li>key와 value는 attention이 이루어지는 위치에 상관없이 같은 값을 갖게 됩니다. 이때 query와 key에 대한 dot-product를 계산하면 각각의 query와 key 사이의 <strong>유사도</strong>를 구할 수 있게 됩니다. 흔히 들어본 cosine similarity는 dot-product에서 vector의 magnitude로 나눈 것입니다. <script type="math/tex">\sqrt{d_k}</script>로 scaling을 해주는 이유는 dot-products의 값이 커질수록 softmax 함수에서 기울기의 변화가 거의 없는 부분으로 가기 때문입니다.</li> <li>softmax를 거친 값을 value에 곱해준다면, query와 유사한 value일수록, 즉 중요한 value일수록 더 높은 값을 가지게 됩니다. 중요한 정보에 더 관심을 둔다는 attention의 원리에 알맞은 것입니다.</li> </ul> <h3 id="multi-head-attention">Multi-Head Attention</h3> <p><img src="/assets/images/mha.PNG" alt="multi-head-attention" /></p> <ul> <li>위의 그림을 수식으로 나타내면 다음과 같습니다.</li> </ul> <script type="math/tex; mode=display">MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O</script> <p>​ where <script type="math/tex">head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)</script></p> <p><img src="/assets/images/multi head.png" alt="multi-head" /></p> <ul> <li><script type="math/tex">d_{model}</script> dimension의 key, value, query들로 하나의 attention을 수행하는 대신 key, value, query들에 각각 다른 학습된 linear projection을 h번 수행하는 게 더 좋다고 합니다. 즉, 동일한 <script type="math/tex">Q, K, V</script>에 각각 다른 weight matrix <script type="math/tex">W</script>를 곱해주는 것이죠. 이때 parameter matrix는 <script type="math/tex">W_i^Q \in \mathbb{R}^{d_{model} \mathsf{x} d_k}, W_i^K \in \mathbb{R}^{d_{model} \mathsf{x} d_k}, W_i^V \in \mathbb{R}^{d_{model} \mathsf{x} d_v}, W_i^O \in \mathbb{R}^{hd_ v\mathsf{x} d_{model}}</script> 입니다.</li> <li>순서대로 query, key, value, output에 대한 parameter matrix입니다. projection이라고 하는 이유는 각각의 값들이 parameter matrix와 곱해졌을 때 <script type="math/tex">d_k, d_v, d_{model}</script>차원으로 project되기 때문입니다. 논문에서는 <script type="math/tex">d_k=d_v=d_{model}/h</script>를 사용했는데 꼭 <script type="math/tex">d_k</script>와 <script type="math/tex">d_v</script>가 같을 필요는 없습니다.</li> <li>이렇게 project된 key, value, query들은 병렬적으로 attention function을 거쳐 <script type="math/tex">d_v</script> dimension output 값으로 나오게 됩니다.</li> <li>그 다음 여러 개의 <script type="math/tex">head</script>를 concatenate하고 다시 projection을 수행합니다. 그래서 최종적인 <script type="math/tex">d_{model}</script> dimension output 값이 나오게 되는거죠.</li> <li>각각의 과정에서 dimension을 표현하면 아래와 같습니다.</li> </ul> <p><img src="/assets/images/차원.png" alt="dimension" /></p> <p>​ *<script type="math/tex">d_Q,d_K,d_V</script>는 각각 query, key, value 개수</p> <h3 id="self-attention">Self-Attention</h3> <h4 id="encoder-self-attention-layer">encoder self-attention layer</h4> <p><img src="/assets/images/encoder self attention.png" alt="encoder-self-attention" /></p> <ul> <li>key, value, query들은 모두 encoder의 이전 layer의 output에서 옵니다. 따라서 이전 layer의 모든 position에 attention을 줄 수 있습니다. 만약 첫번째 layer라면 positional encoding이 더해진 input embedding이 됩니다.</li> </ul> <h4 id="decoder-self-attention-layer">decoder self-attention layer</h4> <p><img src="/assets/images/decoder self attention.png" alt="decoder-self-attention" /></p> <ul> <li>encoder와 비슷하게 decoder에서도 self-attention을 줄 수 있습니다. 하지만 <script type="math/tex">i</script>번째 output을 다시 <script type="math/tex">i+1</script>번째 input으로 사용하는 <strong>auto-regressive</strong>한 특성을 유지하기 위해 , <strong>masking out</strong>된 scaled dot-product attention을 적용했습니다.</li> <li>masking out이 됐다는 것은 <script type="math/tex">i</script>번째 position에 대한 attention을 얻을 때, <script type="math/tex">i</script>번째 이후에 있는 모든 position은 <script type="math/tex">Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>에서 softmax의 input 값을 <script type="math/tex">-\infty</script>로 설정한 것입니다. 이렇게 한다면, <script type="math/tex">i</script>번째 이후에 있는 position에 attention을 주는 경우가 없겠죠.</li> </ul> <h3 id="encoder-decoder-attention-layer">Encoder-Decoder Attention Layer</h3> <p><img src="/assets/images/encoder-decoder attention.png" alt="encoder-decoder-attention" /></p> <ul> <li>query들은 이전 decoder layer에서 오고 key와 value들은 encoder의 output에서 오게 됩니다. 그래서 decoder의 모든 position에서 input sequence 즉, encoder output의 모든 position에 attention을 줄 수 있게 됩니다.</li> <li>query가 decoder layer의 output인 이유는 <em>query</em>라는 것이 조건에 해당하기 때문입니다. 좀 더 풀어서 설명하면, ‘지금 decoder에서 이런 값이 나왔는데 무엇이 output이 돼야 할까?’가 query인 것이죠.</li> <li> <p>이때 query는 이미 이전 layer에서 masking out됐으므로, i번째 position까지만 attention을 얻게 됩니다.이 같은 과정은 sequence-to-sequence의 전형적인 encoder-decoder mechanisms를 따라한 것입니다.</p> <p>*모든 position에서 attention을 줄 수 있다는 게 이해가 안되면 <a href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/">링크</a>를 참고하시기 바랍니다.</p> <p>​</p> </li> </ul> <h2 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h2> <ul> <li>encoder와 decoder의 각각의 layer는 아래와 같은 <strong>fully connected feed-forward network</strong>를 포함하고 있습니다.</li> </ul> <p><img src="/assets/images/Sample-of-a-feed-forward-neural-network.png" alt="feed-forward-network" /></p> <ul> <li>position 마다, 즉 개별 단어마다 적용되기 때문에 <strong>position-wise</strong>입니다. network는 두 번의 linear transformation과 activation function ReLU로 이루어져 있습니다.</li> </ul> <script type="math/tex; mode=display">FFN(x)=max(0, xW_1+b_1)W_2+b_2</script> <p><img src="/assets/images/ffn.png" alt="ffn" /></p> <ul> <li><script type="math/tex">x</script>에 linear transformation을 적용한 뒤, <script type="math/tex">ReLU(max(0, z))</script>를 거쳐 다시 한번 linear transformation을 적용합니다.</li> <li>이때 각각의 position마다 같은 parameter <script type="math/tex">W, b</script>를 사용하지만, layer가 달라지면 다른 parameter를 사용합니다.</li> <li>kernel size가 1이고 channel이 layer인 convolution을 두 번 수행한 것으로도 위 과정을 이해할 수 있습니다.</li> </ul> <p>​</p> <h2 id="positional-encoding">Positional Encoding</h2> <ul> <li>transfomer는 recurrence도 아니고 convolution도 아니기 때문에, 단어의sequence를 이용하기 위해서는 단어의 position에 대한 정보를 추가해줄 필요가 있었습니다.</li> <li>그래서 encoder와 decoder의 input embedding에 <strong>positional encoding</strong>을 더해줬습니다.</li> <li>positional encoding은 <script type="math/tex">d_{model}</script>(embedding 차원)과 같은 차원을 갖기 때문에 positional encoding vector와 embedding vector는 더해질 수 있습니다.</li> <li> <p>논문에서는 다른 *frequency를 가지는 sine과 cosine 함수를 이용했습니다.</p> <p>*주어진 구간내에서 완료되는 cycle의 개수</p> </li> </ul> <script type="math/tex; mode=display">PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})</script> <script type="math/tex; mode=display">PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})</script> <ul> <li><script type="math/tex">pos</script>는 position ,<script type="math/tex">i</script>는 dimension 이고 주기가 <script type="math/tex">10000^{2i/d_{model}}\cdot2\pi</script>인 삼각 함수입니다. 즉, <script type="math/tex">pos</script>는 sequence에서 단어의 위치이고 해당 단어는 <script type="math/tex">i</script>에 0부터 <script type="math/tex">\frac{d_{model}}{2}</script>까지를 대입해 <script type="math/tex">d_{model}</script>차원의 positional encoding vector를 얻게 됩니다. <script type="math/tex">k=2i+1</script>일 때는 cosine 함수를, <script type="math/tex">k=2i</script>일 때는 sine 함수를 이용합니다. 이렇게 positional encoding vector를 <script type="math/tex">pos</script>마다 구한다면 비록 같은 column이라고 할지라도 <script type="math/tex">pos</script>가 다르다면 다른 값을 가지게 됩니다. 즉, <script type="math/tex">pos</script>마다 다른 <script type="math/tex">pos</script>와 구분되는 positional encoding 값을 얻게 되는 것입니다.</li> </ul> <script type="math/tex; mode=display">PE_{pos}=[cos(pos/1), sin(pos/10000^{2/d_{model}}),cos(pos/10000)^{2/d_{model}},...,sin(pos/10000)]</script> <ul> <li> <p>이때 <script type="math/tex">PE_{pos+k}</script>는 <script type="math/tex">PE_{pos}</script>의 linear function으로 나타낼 수 있습니다. 표기를 간단히 하기 위해 <script type="math/tex">c=10000^{\frac{2i}{d_{model}}}</script>라고 해봅시다. <script type="math/tex">sin(a+b)=sin(a)cos(b)+cos(a)sin(b)</script>이고 <script type="math/tex">cos(a + b) = cos (a )cos (b) − sin(a) sin (b)</script> 이므로 다음이 성립합니다.</p> <script type="math/tex; mode=display">PE_{(pos, 2i)}=sin(\frac{pos}{c})</script> </li> </ul> <script type="math/tex; mode=display">PE_{(pos, 2i+1)}=cos(\frac{pos}{c})</script> <script type="math/tex; mode=display">PE_{(pos+k, 2i)}=sin(\frac{pos+k}{c})=sin(\frac{pos}{c})cos(\frac{k}{c})+cos(\frac{pos}{c})sin(\frac{k}{c}) =PE_{(pos,2i)}cos(\frac{k}{c})+cos(\frac{pos}{c})sin(\frac{k}{c})</script> <script type="math/tex; mode=display">PE_{(pos+k, 2i+1)}=cos(\frac{pos+k}{c})=cos(\frac{pos}{c})cos(\frac{k}{c})-sin(\frac{pos}{c})sin(\frac{k}{c}) =PE_{(pos,2i+1)}cos(\frac{k}{c})-sin(\frac{pos}{c})sin(\frac{k}{c})</script> <ul> <li>이런 성질 때문에 model이 relative position에 의해 attention하는 것을 더 쉽게 배울 수 있습니다.</li> <li>논문에서는 학습된 positional embedding 대신 sinusoidal version을 선택했습니다. 만약 학습된 positional embedding을 사용할 경우 training보다 더 긴 sequence가 inference시에 입력으로 들어온다면 문제가 되지만 sinusoidal의 경우 constant하기 때문에 문제가 되지 않습니다. 그냥 좀 더 많은 값을 계산하기만 하면 되는거죠.</li> </ul> <p>​</p> <h1 id="training">Training</h1> <ul> <li>training에 사용된 기법들을 알아보겠습니다.</li> </ul> <h2 id="optimizer">Optimizer</h2> <ul> <li>많이 쓰이는 Adam optimizer를 사용했습니다.</li> <li>특이한 점은 learning rate를 training동안 고정시키지 않고 다음 식에 따라 변화시켰다는 것입니다.</li> </ul> <script type="math/tex; mode=display">lrate = d_{model}^{-0.5}\cdot min(step\_num^{-0.5},step\_num \cdot warmup\_steps^{-1.5})</script> <p><img src="/assets/images/lr graph.png" alt="lr-graph" /></p> <ul> <li><script type="math/tex">warmup\_step</script>까지는 linear하게 learning rate를 증가시키다가, <script type="math/tex">warmup\_step</script> 이후에는 <script type="math/tex">step\_num</script>의 inverse square root에 비례하도록 감소시킵니다.</li> <li>이렇게 하는 이유는 처음에는 학습이 잘 되지 않은 상태이므로 learning rate를 빠르게 증가시켜 변화를 크게 주다가, 학습이 꽤 됐을 시점에 learning rate를 천천히 감소시켜 변화를 작게 주기 위해서입니다.</li> </ul> <p>​</p> <h2 id="regularization">Regularization</h2> <h3 id="residual-connection">Residual Connection</h3> <ul> <li><a href="https://arxiv.org/abs/1603.05027">Identity Mappings in Deep Residual Networks</a>라는 논문에서 제시된 방법이고, 아래의 수식이 residual connection을 나타낸 것입니다.</li> </ul> <script type="math/tex; mode=display">y_l = h(x_l) + F(x_l, W_l)</script> <script type="math/tex; mode=display">x_{l+1} = f(y_l)</script> <ul> <li>이때 <script type="math/tex">h(x_l)=x_l</script>입니다. 논문 제목에서 나온 것처럼 identity mapping을 해주는 것이죠.</li> <li>특정한 위치에서의 <script type="math/tex">x_L</script>을 다음과 같이 <script type="math/tex">x_l</script>과 residual 함수의 합으로 표시할 수 있습니다.</li> </ul> <script type="math/tex; mode=display">x_2 =x_1+F(x_1,W_1)</script> <script type="math/tex; mode=display">x_3 =x_2+F(x_2,W_2)=x_1+F(x_1,W_1)+F(x_2,W_2)</script> <script type="math/tex; mode=display">x_L = x_l+\sum^{L-1}_{i=1} F(x_i, W_i)</script> <ul> <li>그리고 미분을 한다면 다음과 같이 됩니다.</li> </ul> <script type="math/tex; mode=display">\frac{\sigma\epsilon}{\sigma x_l}= \frac{\sigma\epsilon}{\sigma x_L} \frac{\sigma x_L}{\sigma x_l} = \frac{\sigma\epsilon}{\sigma x_L} (1+\frac{\sigma}{\sigma x_l}\sum^{L-1}_{i=1} F(x_i, W_i))</script> <ul> <li>이때, <script type="math/tex">\frac{\sigma\epsilon}{\sigma x_L}</script>은 상위 layer의 gradient 값이 변하지 않고 그대로 하위 layer에 전달되는 것을 보여줍니다. 즉, layer를 거칠수록 gradient가 사라지는 vanishing gradient 문제를 완화해주는 것입니다.</li> <li>또한 forward path나 backward path를 간단하게 표현할 수 있게 됩니다.</li> </ul> <h3 id="layer-normalization">Layer Normalization</h3> <ul> <li><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>이라는 논문에서 제시된 방법입니다.</li> </ul> <script type="math/tex; mode=display">\mu^l = \frac{1}{H}\sum_{i=1}^Ha^l_i</script> <script type="math/tex; mode=display">\sigma^l = \sqrt{\frac{1}{H}\sum_{i=1}^H(a^l_i-\mu^l)^2}</script> <ul> <li>같은 layer에 있는 모든 hidden unit은 동일한 <script type="math/tex">\mu</script>와 <script type="math/tex">\sigma</script>를 공유합니다.</li> <li>그리고 현재 input <script type="math/tex">x^t</script>, 이전의 hidden state <script type="math/tex">h^{t-1}</script>, <script type="math/tex">a^t = W_{hh}h^{t-1}+W_{xh}x^t</script>, parameter <script type="math/tex">g, b</script>가 있을 때 다음과 같이 normalization을 해줍니다.</li> </ul> <script type="math/tex; mode=display">h^t = f[\frac{g}{\sigma^t}\odot(a^t-\mu^t)+b]</script> <ul> <li> <p>이렇게 한다면, gradient가 exploding하거나 vanishing하는 문제를 완화시키고 gradient 값이 안정적인 값을 가짐로 더 빨리 학습을 시킬 수 있습니다.</p> <p>​ (논문에서 recurrent를 기준으로 설명했으므로 이에 따랐습니다.)</p> </li> </ul> <h3 id="dropout">Dropout</h3> <ul> <li> <p><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Dropout: a simple way to prevent neural networks from overfitting</a>라는 논문에서 제시된 방법입니다.</p> <p><img src="/assets/images/dropout.PNG" alt="dropout" /></p> </li> <li> <p><strong>dropout</strong>이라는 용어는 neural network에서 unit들을 dropout하는 것을 가리킵니다. 즉, 해당 unit을 network에서 일시적으로 제거하는 것입니다. 그래서 다른 unit과의 모든 connection이 사라지게 됩니다. 어떤 unit을 dropout할지는 random하게 정합니다.</p> </li> <li> <p>dropout은 training data에 overfitting되는 문제를 어느정도 막아줍니다. dropout된 unit들은 training되지 않는 것이니 training data에 값이 조정되지 않기 때문입니다.</p> </li> </ul> <h3 id="label-smoothing">Label Smoothing</h3> <ul> <li><a href="https://arxiv.org/pdf/1512.00567.pdf">Rethinking the inception architecture for computer vision</a>라는 논문에서 제시된 방법입니다.</li> <li>training동안 실제 정답인 label의 logit은 다른 logit보다 훨씬 큰 값을 갖게 됩니다. 이렇게 해서 model이 주어진 input <script type="math/tex">x</script>에 대한 label <script type="math/tex">y</script>를 맞추는 것이죠.</li> <li>하지만 이렇게 된다면 문제가 발생합니다. overfitting될 수도 있고 가장 큰 logit을 가지는 것과 나머지 사이의 차이를 점점 크게 만들어버립니다. 결국 model이 다른 data에 적응하는 능력을 감소시킵니다.</li> <li>model이 덜 confident하게 만들기 위해, label distribution <script type="math/tex">q(k \mid x)=\delta_{k,y}</script>를 (k가 y일 경우 1, 나머지는 0) 다음과 같이 대체할 수 있습니다.</li> </ul> <script type="math/tex; mode=display">q'(k|x)=(1-\epsilon)\delta_{k,y}+\epsilon u(k)</script> <ul> <li>각각 label에 대한 분포 <script type="math/tex">u(k)</script>, smooting parameter <script type="math/tex">\epsilon</script>입니다. 위와 같다면, k=y인 경우에도 model은 <script type="math/tex">p(y \mid x)=1</script>이 아니라 <script type="math/tex">p(y \mid x)=(1-\epsilon)</script>이 되겠죠. 100%의 확신이 아닌 그보다 덜한 확신을 하게 되는 것입니다.</li> </ul> <p>​</p> <h1 id="conclusion">Conclusion</h1> <ul> <li>transformer는 recurrence를 이용하지 않고도 빠르고 정확하게 sequential data를 처리할 수 있는 model로 제시되었습니다.</li> <li>여러가지 기법이 사용됐지만, 가장 핵심적인 것은 encoder와 decoder에서 attention을 통해 query와 가장 밀접한 연관성을 가지는 value를 강조할 수 있고 병렬화가 가능해진 것입니다.</li> </ul> <p>​</p> <h2 id="reference">Reference</h2> <ul> <li>http://www.whydsp.org/280</li> <li>http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/</li> <li>http://openresearch.ai/t/identity-mappings-in-deep-residual-networks/47</li> <li>https://m.blog.naver.com/PostView.nhn?blogId=laonple&amp;logNo=220793640991&amp;proxyReferer=https%3A%2F%2Fwww.google.co.kr%2F</li> <li>https://www.researchgate.net/figure/Sample-of-a-feed-forward-neural-network_fig1_234055177</li> <li>https://arxiv.org/abs/1603.05027</li> <li>https://arxiv.org/abs/1607.06450</li> <li>http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf</li> <li>https://arxiv.org/pdf/1512.00567.pdf</li> </ul> <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <!-- Global site tag (gtag.js) - Google Analytics --> <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-103074382-1"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-103074382-1'); </script> <div class="blog-navigation"> <a class="prev" href="http://localhost:4000/Activation_Function/">&laquo; Activation Function</a> </div> <div class="related"> </div> <section class="author"> <div class="toleft"> <img class="selfie" src="http://localhost:4000/" alt="" alt="author selfie"> </div> <div class="toright"> <h4 class="name"></h4> <p class="bio"></p> <div class="share"> <!-- Note: Only use three share links if your site width is set to large --> <!-- If site width is set to normal, you may choose any two share links --> <a class="twitter" href="https://twitter.com/intent/tweet?text=http://localhost:4000/transformer/ - Attention is all you need paper 뽀개기 by @"> <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg><span class="icon-twitter">Tweet</span> </a> <a class="facebook" href="javascript:void(0)" onclick="window.open('https://facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href), 'facebook-share-dialog', 'width=626,height=436'); return false;"> <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg><span class="icon-facebook-rect">Share</span> </a> <!-- <a class="google-plus" href="https://plus.google.com/share?url=http://localhost:4000/transformer/" target="_blank"> <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg><span class="icon-google-plus">Share</span> </a> --> <!-- <a class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://localhost:4000/transformer/&amp;title=Attention is all you need paper 뽀개기&amp;summary=Transformer paper review&amp;source=http://localhost:4000" target="_blank"> <svg class="icon icon-linkedin"><use xlink:href="#icon-linkedin"></use></svg><span class="icon-linkedin">Share</span> </a> --> <!-- <a class="reddit" href="https://reddit.com/submit?url=http://localhost:4000/transformer/&amp;title=Attention is all you need paper 뽀개기" target="_blank"> <svg class="icon icon-reddit"><use xlink:href="#icon-reddit"></use></svg><span class="icon-reddit">Share</span> </a> --> </div> </div> </section> <section class="disqus"> <div id="disqus_thread"></div> <script type="text/javascript"> var disqus_config = function () { this.page.url = 'http://localhost:4000/transformer/'; this.page.identifier = '/transformer'; }; var disqus_shortname = 'pozalabs'; var disqus_developer = 0; (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a> </section> <footer class="footer-main"> 포자랩스의 기술 블로그 © 2018 <a class="link" href="http://localhost:4000/feed.xml" target="_blank"><svg class="icon icon-rss"><use xlink:href="#icon-rss"></use></svg></a> <p class="extra"> <a class="link" href="https://github.com/sergiokopplin/indigo">Indigo theme</a> by <a class="link" href="http://koppl.in">Kopplin</a> </p> </footer> </div> </div> <svg display="none" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"> <defs> <symbol id="icon-rss" viewBox="0 0 1024 1024"> <title>rss</title> <path class="path1" d="M122.88 122.88v121.19c362.803 0 656.896 294.195 656.896 656.998h121.293c0-429.773-348.416-778.189-778.189-778.189zM122.88 365.414v121.293c228.813 0 414.362 185.498 414.362 414.413h121.242c0-295.834-239.821-535.706-535.603-535.706zM239.053 668.621c-64.205 0-116.224 52.122-116.224 116.275s52.019 116.224 116.224 116.224 116.173-52.019 116.173-116.224-51.968-116.275-116.173-116.275z"></path> </symbol> <symbol id="icon-facebook" viewBox="0 0 1024 1024"> <title>facebook</title> <path class="path1" d="M870.4 51.2h-716.8c-56.32 0-102.4 46.080-102.4 102.4v716.8c0 56.371 46.080 102.4 102.4 102.4h358.4v-358.4h-102.4v-126.72h102.4v-104.96c0-110.797 62.054-188.621 192.819-188.621l92.314 0.102v133.376h-61.286c-50.893 0-70.246 38.195-70.246 73.626v86.528h131.482l-29.082 126.669h-102.4v358.4h204.8c56.32 0 102.4-46.029 102.4-102.4v-716.8c0-56.32-46.080-102.4-102.4-102.4z"></path> </symbol> <symbol id="icon-twitter" viewBox="0 0 1024 1024"> <title>twitter</title> <path class="path1" d="M886.579 319.795c0.41 8.294 0.563 16.691 0.563 24.986 0 255.488-194.406 549.99-549.888 549.99-109.21 0-210.739-32-296.294-86.886 15.155 1.792 30.515 2.714 46.080 2.714 90.624 0 173.926-30.925 240.026-82.688-84.531-1.587-155.955-57.395-180.531-134.195 11.776 2.202 23.91 3.379 36.352 3.379 17.664 0 34.765-2.304 50.944-6.707-88.422-17.818-155.034-95.898-155.034-189.594 0-0.819 0-1.587 0-2.406 26.061 14.49 55.91 23.194 87.552 24.218-51.866-34.714-86.016-93.798-86.016-160.922 0-35.379 9.523-68.608 26.214-97.178 95.283 116.992 237.773 193.894 398.387 201.984-3.277-14.182-4.966-28.877-4.966-44.083 0-106.701 86.477-193.178 193.229-193.178 55.603 0 105.83 23.398 141.107 60.979 43.981-8.704 85.35-24.781 122.726-46.899-14.438 45.107-45.107 82.995-84.992 106.906 39.117-4.71 76.288-15.002 111.002-30.413-25.907 38.81-58.675 72.806-96.461 99.994z"></path> </symbol> <symbol id="icon-github" viewBox="0 0 1024 1024"> <title>github</title> <path class="path1" d="M674.816 579.021c-36.762 0-66.56 41.318-66.56 92.109 0 50.893 29.798 92.211 66.56 92.211s66.56-41.318 66.56-92.211c-0.051-50.79-29.798-92.109-66.56-92.109zM906.547 339.251c7.629-18.688 7.936-124.877-32.512-226.611 0 0-92.723 10.189-233.011 106.496-29.44-8.192-79.258-12.186-128.973-12.186-49.818 0-99.584 3.994-129.024 12.186-140.339-96.307-233.062-106.496-233.062-106.496-40.397 101.734-39.987 207.923-32.461 226.611-47.514 51.61-76.544 113.613-76.544 198.195 0 367.923 305.306 373.811 382.31 373.811 17.51 0 52.122 0.102 88.781 0.102 36.608 0 71.27-0.102 88.678-0.102 77.107 0 382.31-5.888 382.31-373.811 0-84.582-28.979-146.586-76.493-198.195zM513.434 866.048h-2.867c-193.075 0-343.501-22.989-343.501-210.688 0-45.005 15.872-86.682 53.606-121.293 62.822-57.702 169.216-27.187 289.894-27.187 0.512 0 1.024 0 1.485 0 0.512 0 0.922 0 1.382 0 120.678 0 227.123-30.515 289.997 27.187 37.632 34.611 53.504 76.288 53.504 121.293 0 187.699-150.374 210.688-343.501 210.688zM349.235 579.021c-36.762 0-66.56 41.318-66.56 92.109 0 50.893 29.798 92.211 66.56 92.211 36.813 0 66.611-41.318 66.611-92.211 0-50.79-29.798-92.109-66.611-92.109z"></path> </symbol> <symbol id="icon-youtube" viewBox="0 0 1024 1024"> <title>youtube</title> <path class="path1" d="M512 117.76c-503.194 0-512 44.749-512 394.24s8.806 394.24 512 394.24 512-44.749 512-394.24-8.806-394.24-512-394.24zM676.096 529.101l-229.888 107.315c-20.122 9.318-36.608-1.126-36.608-23.347v-202.138c0-22.17 16.486-32.666 36.608-23.347l229.888 107.315c20.122 9.421 20.122 24.781 0 34.202z"></path> </symbol> <symbol id="icon-mail" viewBox="0 0 1024 1024"> <title>mail</title> <path class="path1" d="M80.589 270.643c24.986 13.414 371.098 199.373 384 206.285s29.594 10.189 46.387 10.189c16.794 0 33.485-3.277 46.387-10.189s359.014-192.87 384-206.285c25.037-13.466 48.691-65.843 2.765-65.843h-866.253c-45.926 0-22.272 52.378 2.714 65.843zM952.986 383.437c-28.416 14.797-378.214 197.069-395.622 206.182s-29.594 10.189-46.387 10.189-28.979-1.075-46.387-10.189-365.21-191.437-393.626-206.234c-19.968-10.445-19.763 1.792-19.763 11.213s0 373.402 0 373.402c0 21.504 28.979 51.2 51.2 51.2h819.2c22.221 0 51.2-29.696 51.2-51.2 0 0 0-363.93 0-373.35s0.205-21.658-19.814-11.213z"></path> </symbol> <symbol id="icon-spotify" viewBox="0 0 1024 1024"> <title>spotify</title> <path class="path1" d="M512 61.44c-248.883 0-450.56 201.626-450.56 450.56 0 248.781 201.677 450.56 450.56 450.56 248.934 0 450.509-201.728 450.509-450.56 0-248.883-201.523-450.56-450.509-450.56zM690.074 742.502c-8.858 0-15.053-3.379-21.555-7.322-60.877-36.915-136.294-56.269-218.010-56.269-41.677 0-86.682 4.966-133.632 14.592l-5.734 1.434c-5.939 1.434-12.032 3.021-16.691 3.021-18.995 0-33.843-14.746-33.843-33.587 0-19.098 10.752-32.614 28.774-35.994 56.115-12.8 108.954-19.046 161.382-19.046 94.976 0 179.866 22.016 252.467 65.485 12.442 7.27 20.275 15.667 20.275 34.202-0.051 18.483-15.002 33.485-33.434 33.485zM736.819 611.379c-10.598 0-17.562-4.045-23.706-7.629-109.722-65.075-273.050-86.682-407.603-50.842-2.253 0.666-4.301 1.28-6.144 1.894-5.069 1.587-9.779 3.174-16.435 3.174-22.118 0-40.090-18.074-40.090-40.346 0-21.453 11.213-36.454 31.437-42.189 51.866-14.234 100.557-23.654 170.65-23.654 113.254 0 223.078 28.416 309.146 79.923 15.667 8.96 22.784 21.197 22.784 39.475 0 22.221-17.971 40.192-40.038 40.192zM789.862 461.875c-9.984 0-16.128-2.406-25.344-7.373-74.394-44.646-190.464-71.219-310.733-71.219-62.669 0-119.603 6.912-169.267 20.326-1.69 0.41-3.277 0.87-5.018 1.382-5.274 1.587-11.878 3.482-18.688 3.482-26.419 0-47.053-20.89-47.053-47.565 0-23.194 13.005-40.909 34.816-47.36 59.955-17.715 128.973-26.675 205.107-26.675 137.114 0 267.571 30.464 357.939 83.507 16.998 9.677 25.344 24.32 25.344 44.646 0 26.266-20.685 46.848-47.104 46.848z"></path> </symbol> <symbol id="icon-lastfm" viewBox="0 0 315 315"> <title>lastfm</title> <path class="path1" d="M264.467 135.355c-2.688-0.92-5.289-1.773-7.787-2.594C236.855 126.26 230 123.449 230 112.41 c0-9.572 6.799-16.26 16.533-16.26c7.986 0 13.502 3.307 19.039 11.41c2.156 3.158 6.348 4.188 9.721 2.389l19.148-10.205 c1.762-0.938 3.076-2.541 3.652-4.453c0.576-1.91 0.367-3.973-0.582-5.729c-11.123-20.596-27.912-31.037-49.9-31.037 c-16.592 0-30.648 5.227-40.654 15.117c-9.918 9.803-15.16 23.453-15.16 39.471c0 33.607 21.297 47.508 58.063 60.156 c21.045 7.311 25.965 10.137 25.965 21.121c0 13.578-11.727 23.434-27.885 23.434c-0.486 0-0.98-0.008-1.48-0.025 c-17.377-0.607-22.725-9.088-30.789-28.297c-12.947-30.814-28.082-67.734-29.205-70.543c-0.012-0.031-0.025-0.064-0.037-0.096 c-16.416-39.535-49.057-62.209-89.555-62.209C43.457 56.654 0 101.9 0 157.518c0 55.598 43.457 100.828 96.873 100.828 c29.217 0 56.559-13.49 75.016-37.014c1.674-2.133 2.064-5.004 1.025-7.508l-11.541-27.781c-1.125-2.711-3.729-4.514-6.66-4.619 c-2.945-0.105-5.654 1.512-6.971 4.135c-9.977 19.9-29.469 32.262-50.869 32.262c-31.658 0-57.414-27.053-57.414-60.303 c0-33.26 25.756-60.32 57.414-60.32c23.029 0 44.1 14.273 52.432 35.516c0.023 0.055 0.045 0.111 0.068 0.166l28.574 67.982 l3.293 7.617c13.811 33.602 34.273 48.652 66.359 48.797h0.133c38.348 0 67.268-26.699 67.268-62.103 C315 159.674 295.66 145.965 264.467 135.355z"></path> </symbol> <symbol id="icon-instagram" viewBox="0 0 1024 1024"> <title>instagram</title> <path class="path1" d="M870.4 51.2h-716.8c-56.32 0-102.4 46.080-102.4 102.4v716.8c0 56.371 46.080 102.4 102.4 102.4h716.8c56.32 0 102.4-46.029 102.4-102.4v-716.8c0-56.32-46.080-102.4-102.4-102.4zM511.181 794.778c156.621 0 283.546-127.027 283.546-283.597 0-17.306-2.202-33.997-5.274-50.381h80.947v369.459c0 19.558-15.872 35.328-35.482 35.328h-645.837c-19.61 0-35.482-15.77-35.482-35.328v-369.459h79.309c-3.123 16.384-5.325 33.075-5.325 50.381 0 156.621 127.027 283.597 283.597 283.597zM333.978 511.181c0-97.894 79.36-177.203 177.254-177.203 97.843 0 177.254 79.309 177.254 177.203s-79.411 177.254-177.254 177.254c-97.946 0-177.254-79.36-177.254-177.254zM834.918 307.2h-82.688c-19.558 0-35.43-15.974-35.43-35.43v-82.79c0-19.558 15.872-35.379 35.379-35.379h82.688c19.661 0 35.533 15.821 35.533 35.379v82.739c0 19.507-15.872 35.482-35.482 35.482z"></path> </symbol> <symbol id="icon-linkedin" viewBox="0 0 1024 1024"> <title>linkedin</title> <path class="path1" d="M256 153.6c0 54.374-36.352 101.171-102.451 101.171-62.208 0-102.349-44.134-102.349-98.509 0-55.808 38.912-105.062 102.4-105.062s101.171 46.592 102.4 102.4zM51.2 972.8v-665.6h204.8v665.6h-204.8z"></path> <path class="path2" d="M358.4 534.733c0-79.104-2.611-145.203-5.222-202.291h184.013l9.114 88.218h3.891c25.907-41.523 89.395-102.4 195.686-102.4 129.638 0 226.918 86.784 226.918 273.51v381.030h-204.8v-351.283c0-81.613-31.078-143.872-102.4-143.872-54.374 0-81.613 44.032-95.898 80.333-5.222 13.005-6.502 31.13-6.502 49.306v365.517h-204.8v-438.067z"></path> </symbol> <symbol id="icon-google" viewBox="0 0 1024 1024"> <title>google</title> <path class="path1" d="M522.2 438.8v175.6h290.4c-11.8 75.4-87.8 220.8-290.4 220.8-174.8 0-317.4-144.8-317.4-323.2s142.6-323.2 317.4-323.2c99.4 0 166 42.4 204 79l139-133.8c-89.2-83.6-204.8-134-343-134-283 0-512 229-512 512s229 512 512 512c295.4 0 491.6-207.8 491.6-500.2 0-33.6-3.6-59.2-8-84.8l-483.6-0.2z"></path> </symbol> <symbol id="icon-google-plus" viewBox="0 0 1317 1024"> <title>google-plus</title> <path class="path1" d="M821.143 521.714q0 118.857-49.714 211.714t-141.714 145.143-210.857 52.286q-85.143 0-162.857-33.143t-133.714-89.143-89.143-133.714-33.143-162.857 33.143-162.857 89.143-133.714 133.714-89.143 162.857-33.143q163.429 0 280.571 109.714l-113.714 109.143q-66.857-64.571-166.857-64.571-70.286 0-130 35.429t-94.571 96.286-34.857 132.857 34.857 132.857 94.571 96.286 130 35.429q47.429 0 87.143-13.143t65.429-32.857 44.857-44.857 28-47.429 12.286-42.286h-237.714v-144h395.429q6.857 36 6.857 69.714zM1316.571 452v120h-119.429v119.429h-120v-119.429h-119.429v-120h119.429v-119.429h120v119.429h119.429z"></path> </symbol> <symbol id="icon-pinterest" viewBox="0 0 1024 1024"> <title>pinterest</title> <path class="path1" d="M512 68.4c-245 0-443.6 198.6-443.6 443.6 0 188 117 348.4 282 413-3.8-35-7.4-89 1.6-127.2 8-34.6 52-220.4 52-220.4s-13.2-26.6-13.2-65.8c0-61.6 35.8-107.8 80.2-107.8 37.8 0 56.2 28.4 56.2 62.4 0 38-24.2 95-36.8 147.6-10.6 44.2 22 80.2 65.6 80.2 78.8 0 139.4-83.2 139.4-203.2 0-106.2-76.4-180.4-185.2-180.4-126.2 0-200.2 94.6-200.2 192.6 0 38.2 14.6 79 33 101.2 3.6 4.4 4.2 8.2 3 12.8-3.4 14-10.8 44.2-12.4 50.4-2 8.2-6.4 9.8-14.8 6-55.4-25.8-90-106.8-90-171.8 0-140 101.6-268.4 293-268.4 153.8 0 273.4 109.6 273.4 256.2 0 152.8-96.4 276-230.2 276-45 0-87.2-23.4-101.6-51 0 0-22.2 84.6-27.6 105.4-10 38.6-37 86.8-55.2 116.2 41.6 12.8 85.6 19.8 131.4 19.8 245 0 443.6-198.6 443.6-443.6 0-245.2-198.6-443.8-443.6-443.8z"></path> </symbol> <symbol id="icon-medium" viewBox="0 0 179.2 179.2"> <title>medium</title> <path transform="scale(0.1,-0.1) translate(0,-1536)" d="M597 1115v-1173q0 -25 -12.5 -42.5t-36.5 -17.5q-17 0 -33 8l-465 233q-21 10 -35.5 33.5t-14.5 46.5v1140q0 20 10 34t29 14q14 0 44 -15l511 -256q3 -3 3 -5zM661 1014l534 -866l-534 266v600zM1792 996v-1054q0 -25 -14 -40.5t-38 -15.5t-47 13l-441 220zM1789 1116 q0 -3 -256.5 -419.5t-300.5 -487.5l-390 634l324 527q17 28 52 28q14 0 26 -6l541 -270q4 -2 4 -6z" /> </symbol> <symbol id="icon-vimeo" viewBox="0 0 21 21"> <title>vimeo</title> <path d="M17.811,2.018c2.017,0.053,3.026,1.198,3.036,3.438c0,0.147-0.005,0.3-0.013,0.457c-0.089,1.899-1.502,4.486-4.245,7.76 c-2.829,3.43-5.229,5.147-7.2,5.156c-1.226,0-2.244-1.05-3.061-3.151l-0.858-2.88L4.622,9.922C3.997,7.838,3.329,6.798,2.616,6.798 c-0.156,0-0.697,0.304-1.626,0.91L0,6.537l1.536-1.276l1.511-1.263C4.4,2.914,5.429,2.328,6.135,2.241 c0.094-0.01,0.188-0.013,0.284-0.013c1.449,0,2.354,1.041,2.709,3.124C9.326,6.54,9.49,7.506,9.623,8.248 C9.752,8.992,9.86,9.51,9.946,9.805c0.479,1.97,0.995,2.96,1.55,2.968c0.426,0,1.082-0.642,1.968-1.926 c0.866-1.319,1.332-2.296,1.392-2.932c0.019-0.129,0.026-0.25,0.026-0.362c0-0.861-0.474-1.29-1.418-1.29 c-0.479,0-0.99,0.102-1.537,0.299c0.98-3.021,2.864-4.534,5.65-4.544C17.655,2.018,17.732,2.018,17.811,2.018z"/> </symbol> <symbol id="icon-stackoverflow" viewBox="0 0 878 1024"> <title>stackoverflow</title> <path class="path1" d="M736.571 932.571h-638.857v-274.286h-91.429v365.714h821.714v-365.714h-91.429v274.286zM198.286 633.143l18.857-89.714 447.429 94.286-18.857 89.143zM257.143 419.429l38.286-83.429 414.286 193.714-38.286 82.857zM372 216l58.286-70.286 350.857 293.143-58.286 70.286zM598.857 0l272.571 366.286-73.143 54.857-272.571-366.286zM188.571 840.571v-90.857h457.143v90.857h-457.143z"></path> </symbol> <symbol id="icon-reddit" viewBox="0 0 1024 1024"> <title>reddit</title> <path class="path1" d="M1024 483.429q0 33.143-16.857 60.286t-45.429 41.429q6.857 26.286 6.857 54.857 0 88.571-60.857 164t-166 119.143-228.571 43.714-228.286-43.714-165.714-119.143-60.857-164q0-26.857 6.286-53.714-29.143-14.286-46.857-42t-17.714-60.857q0-46.857 33.143-80.286t80.571-33.429q48.571 0 82.857 36 124.571-86.857 294.286-92.571l66.286-297.714q1.714-7.429 8.571-12t14.857-2.857l210.857 46.286q10.286-21.143 30.857-34t45.143-12.857q35.429 0 60.571 24.857t25.143 60.286-25.143 60.571-60.571 25.143-60.286-24.857-24.857-60.286l-190.857-42.286-59.429 269.714q171.429 5.143 296.571 91.429 33.143-34.857 81.714-34.857 47.429 0 80.571 33.429t33.143 80.286zM238.857 597.143q0 35.429 24.857 60.571t60.286 25.143 60.571-25.143 25.143-60.571-25.143-60.286-60.571-24.857q-34.857 0-60 25.143t-25.143 60zM701.714 800q6.286-6.286 6.286-14.857t-6.286-14.857q-5.714-5.714-14.286-5.714t-14.857 5.714q-23.429 24-69.143 35.429t-91.429 11.429-91.429-11.429-69.143-35.429q-6.286-5.714-14.857-5.714t-14.286 5.714q-6.286 5.714-6.286 14.571t6.286 15.143q24.571 24.571 67.714 38.857t70 16.857 52 2.571 52-2.571 70-16.857 67.714-38.857zM700 682.857q35.429 0 60.286-25.143t24.857-60.571q0-34.857-25.143-60t-60-25.143q-35.429 0-60.571 24.857t-25.143 60.286 25.143 60.571 60.571 25.143z"></path> </symbol> <symbol id="icon-quora" viewBox="0 0 76 76"> <title>quora</title> <path class="path1" d="M67.9,59.1c0,0-0.4,5.3-5.2,5.3c-3.7,0-6.3-2.8-8.7-6.5c6.8-5.9,11.1-14.7,11.1-24.6C65.1,15.4,51.1,1,33.8,1 S2.6,15.4,2.6,33.2s14,32.2,31.3,32.2c3.1,0,6.2-0.5,9-1.4C46.5,69.8,51,75,58.2,75c14.6,0,15.2-15.9,15.2-15.9L67.9,59.1 L67.9,59.1z M33.8,60.2c-10.1,0-18.2-12.1-18.2-26.9S23.8,6.3,33.8,6.3s18.2,12.1,18.2,26.9c0,5.9-1.3,11.4-3.5,15.8 c-2.5-3.5-5.4-6.5-9.7-7.5c-7.5-1.7-14,1.7-16.1,3.4l1.9,4c0,0,2-1.1,6.8,0c3.1,0.7,5.4,4.9,8.1,9.8C37.9,59.7,35.9,60.2,33.8,60.2 z"></path> </symbol> </defs> </svg> <!-- Google Analytics Tracking code --> <!-- <script type="text/javascript"> var _gaq = _gaq || []; _gaq.push(['_setAccount', '']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-82148706-1', 'auto'); ga('send', 'pageview'); </script> </body> </html>
